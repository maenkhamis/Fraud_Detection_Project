# -*- coding: utf-8 -*-
"""task3_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YH9aoOHh_oCggGYdAJfgv_mPUzGdr-H4

**1. Data Loading and Initial Exploration**

This section is for importing the necessary libraries and loading the fraud.csv dataset. The code then checks for missing values to ensure the data is clean before further analysis.
"""

import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('fraud.csv')

# Check for missing values
df.isnull().sum()

"""**2. Feature Engineering and Preprocessing**

This part of the code prepares the data for machine learning models. It converts categorical variables into a numerical format, creates a new hour feature from the step column, and scales the amount column.
"""

# One-hot encode the categorical columns 'age', 'gender', and 'category'
df_encoded = pd.get_dummies(df, columns=['age', 'gender', 'category'], drop_first=True)

# Define features (X) and target (y)
X = df_encoded.drop(columns='fraud')
y = df_encoded['fraud']

# Create a new 'hour' feature from the 'step' column
df['hour'] = df['step'] % 24

# Create a new 'Hour' feature on the encoded DataFrame for modeling
df_encoded['Hour'] = df['step'] % 24

# Scale the 'amount' feature to standardize its range
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df_encoded['amount_scaled'] = scaler.fit_transform(df[['amount']])

# Drop the original 'amount' and other non-useful columns
df_encoded.drop(columns='amount', inplace=True)
X = df_encoded.drop(columns='fraud')
y = df_encoded['fraud']
X = X.drop(columns=['customer', 'merchant', 'zipcodeOri', 'zipMerchant'], errors='ignore')

"""**3. Exploratory Data Analysis (EDA) and Visualization**

These cells are for generating various plots to visualize the data and understand the relationships between features and the target variable (fraud). This helps in gaining insights into the dataset.
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Count of Fraud vs. Non-Fraud
sns.countplot(x='fraud', data=df)
plt.title('Fraud Class Distribution')
plt.show()

# Or in numbers
df['fraud'].value_counts(normalize=True) * 100

# Box plot of transaction amounts by fraud class
sns.boxplot(x='fraud', y='amount', data=df)
plt.title('Transaction Amounts by Fraud Class')
plt.show()

# Count plot of fraud by age group
sns.countplot(x='age', hue='fraud', data=df)
plt.title('Fraud Count by Age Group')
plt.show()

# Histogram of fraud distribution by hour of day
sns.histplot(data=df, x='hour', hue='fraud', multiple='stack', bins=24)
plt.title('Fraud Distribution by Hour of Day')
plt.show()

# Count plot of fraud by category
plt.figure(figsize=(10, 6))
sns.countplot(y='category', hue='fraud', data=df)
plt.title('Fraud Count by Category')
plt.show()

"""**4. Model Training and Evaluation (without SMOTE)**

This section sets up the data for modeling by splitting it into training and testing sets. It then trains and evaluates three different models—Logistic Regression, Random Forest, and XGBoost—to establish a performance baseline.
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Train and evaluate Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
print("Logistic Regression Results:")
print(confusion_matrix(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))
print("AUC-ROC:", roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1]))

# Train and evaluate Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("------------------------------------------")
print("Random Forest Results:")
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))
print("AUC-ROC:", roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1]))

# Train and evaluate XGBoost
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)
print("------------------------------------------")
print("XGBoost Results:")
print(confusion_matrix(y_test, y_pred_xgb))
print(classification_report(y_test, y_pred_xgb))
print("AUC-ROC:", roc_auc_score(y_test, xgb_model.predict_proba(X_test)[:, 1]))

"""**5. Addressing Class Imbalance with SMOTE**

Because the dataset has very few fraudulent transactions, this section uses the SMOTE technique to oversample the minority class, creating a more balanced training dataset.
"""

from imblearn.over_sampling import SMOTE
import collections

# Apply SMOTE on training data only
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Check new class distribution
print("Before SMOTE:", collections.Counter(y_train))
print("After SMOTE: ", collections.Counter(y_train_resampled))

"""**6. Model Training and Evaluation (with SMOTE)**

After balancing the training data, the Logistic Regression and Random Forest models are retrained and re-evaluated to see the impact of SMOTE on their performance.
"""

# Train and evaluate Logistic Regression with SMOTE data
lr_smote = LogisticRegression(max_iter=1000)
lr_smote.fit(X_train_resampled, y_train_resampled)
y_pred_lr_smote = lr_smote.predict(X_test)
print("Logistic Regression (SMOTE) Results:")
print(confusion_matrix(y_test, y_pred_lr_smote))
print(classification_report(y_test, y_pred_lr_smote))
print("AUC-ROC:", roc_auc_score(y_test, lr_smote.predict_proba(X_test)[:, 1]))

# Train and evaluate Random Forest with SMOTE data
rf_smote = RandomForestClassifier(n_estimators=100, random_state=42)
rf_smote.fit(X_train_resampled, y_train_resampled)
y_pred_rf_smote = rf_smote.predict(X_test)
print("Random Forest (SMOTE) Results:")
print(confusion_matrix(y_test, y_pred_rf_smote))
print(classification_report(y_test, y_pred_rf_smote))
print("AUC-ROC:", roc_auc_score(y_test, rf_smote.predict_proba(X_test)[:, 1]))

"""**7. Feature Importance Analysis**

This code identifies the most important features for fraud detection from the Random Forest model and visualizes them.
"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Get feature names and importance scores
feature_names = X_train.columns
importances = rf_smote.feature_importances_

# Combine into a DataFrame and sort by importance
feat_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feat_importance_df = feat_importance_df.sort_values(by='Importance', ascending=False)
feat_importance_df.head(10)  # Top 10 features

# Plot the top 15 features
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feat_importance_df.head(15))
plt.title('Top 15 Important Features for Fraud Detection')
plt.tight_layout()
plt.show()

"""**8. Model Persistence**

The final step is to save the trained model, the scaler, and the feature names to .pkl files. This allows you to use the model later to make predictions on new data without having to retrain it.

Python


"""

import joblib

# Reapply SMOTE and retrain the final model before saving
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
from sklearn.ensemble import RandomForestClassifier
rf_smote = RandomForestClassifier(n_estimators=100, random_state=42)
rf_smote.fit(X_train_resampled, y_train_resampled)

# Save the final model, scaler, and feature names
joblib.dump(rf_smote, 'fraud_rf_model.pkl')
joblib.dump(scaler, 'amount_scaler.pkl')
joblib.dump(X_train.columns.tolist(), 'feature_names.pkl')

"""9. load models and work on a new data"""

import os
import joblib
import pandas as pd
import numpy as np

# --- Configuration: Define the file names of your saved model components ---
MODEL_FILE = 'fraud_rf_model_tuned.pkl'
SCALER_FILE = 'amount_scaler.pkl'
FEATURE_NAMES_FILE = 'feature_names.pkl'

REQUIRED_FILES = [MODEL_FILE, SCALER_FILE, FEATURE_NAMES_FILE]

# --- Step 1: Check for Necessary Files ---
# This ensures that your training script has been run successfully and the model files exist.
missing_files = [f for f in REQUIRED_FILES if not os.path.exists(f)]

if missing_files:
    print(f"Error: The following files are missing: {', '.join(missing_files)}")
    print("Please ensure your training script has been run successfully to generate and save these files before attempting to load them.")
else:
    # --- Step 2: Load Saved Components ---
    # Load the trained model, the StandardScaler object, and the list of feature names.
    print("Loading saved model, scaler, and feature names...")
    best_rf_model = joblib.load(MODEL_FILE)
    scaler = joblib.load(SCALER_FILE)
    feature_names = joblib.load(FEATURE_NAMES_FILE)
    print("Files loaded successfully.")

    # --- Step 3: Create a Dummy Dataset for Prediction ---
    # This simulates a new batch of transactions that your model has not seen before.
    print("Generating a dummy dataset for prediction...")
    np.random.seed(123)

    # Note: These feature names assume a specific structure from your dummy training data.
    # In a real-world scenario, you would replace these with your actual new data.
    new_raw_data = {
        'feature1': np.random.rand(5),
        'feature2': np.random.rand(5),
        'category_A': np.random.randint(0, 2, 5),
        'category_B': np.random.randint(0, 2, 5),
        'amount': np.random.rand(5) * 1500,  # Random amounts for new transactions
        'Hour': np.random.randint(0, 24, 5)   # Random hours
    }
    new_df_raw = pd.DataFrame(new_raw_data)

    # --- Step 4: Preprocess the New Data ---
    # It is crucial to apply the exact same preprocessing steps as during training.
    print("Preprocessing the new data...")
    new_df_raw['amount_scaled'] = scaler.transform(new_df_raw[['amount']])
    new_df_raw.drop(columns='amount', inplace=True)

    # Ensure the columns are in the exact same order as the training data
    new_data_for_prediction = new_df_raw[feature_names]
    print("Data preprocessed and structured correctly.")

    # --- Step 5: Make Predictions ---
    print("Making predictions on the new data...")
    predictions = best_rf_model.predict(new_data_for_prediction)
    probabilities = best_rf_model.predict_proba(new_data_for_prediction)[:, 1]

    # --- Step 6: Display Results ---
    results_df = pd.DataFrame({
        'Transaction_ID': range(1, len(new_data_for_prediction) + 1),
        'Predicted_Class': predictions,
        'Fraud_Probability': probabilities
    })

    results_df['Predicted_Class'] = results_df['Predicted_Class'].apply(lambda x: 'Fraudulent' if x == 1 else 'Not Fraudulent')

    print("\n--- Predictions on New Data ---")
    print(results_df.to_markdown(index=False, numalign="left", stralign="left"))
    print("\n--- Raw Input Data Used for Prediction (First 5 Rows) ---")
    print(new_data_for_prediction.head().to_markdown(index=False, numalign="left", stralign="left"))